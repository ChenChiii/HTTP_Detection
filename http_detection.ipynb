{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP Detection\n",
    "HTTP Detection use machine learning to detect anomalous HTTP request.  \n",
    "Dataset: [CSIC 2010 HTTP Dataset](https://petescully.co.uk/research/csic-2010-http-dataset-in-csv-format-for-weka-analysis/)  \n",
    "In this code, we trained Three models, including random forest model, n-gram CNN model, and n-gram LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import evaluate_model\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Number of CPUs for ensemble learning methods\n",
    "N_ENSEMBLE_CPUS = max(os.cpu_count()//2, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis HTTP Request Attributes\n",
    "We will first combine normal data and anomalous data.  \n",
    "Then we will analyze the value in HTTP requests.  \n",
    "We will find out those values that are same in every request, and these data will not be used to train our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_traffic_dataset = pd.read_csv(\"dataset/normalTrafficTraining.csv\")\n",
    "anomalous_traffic_dataset = pd.read_csv(\"dataset/anomalousTrafficTest.csv\")\n",
    "# Preview the dataset\n",
    "traffic_dataset = pd.concat([normal_traffic_dataset, anomalous_traffic_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute that all have same value in every columns.\n",
    "same_value_columns = []\n",
    "for name, item in traffic_dataset.items():\n",
    "    if len(item.unique()) == 1:\n",
    "        same_value_columns.append(name)\n",
    "print(\"same value columns: \", same_value_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute that all have binary value.\n",
    "binary_columns = []\n",
    "print(\"value in binary column\")\n",
    "for name, item in traffic_dataset.items():\n",
    "    if len(item.unique()) == 2:\n",
    "        print(item.unique())\n",
    "        binary_columns.append(name)\n",
    "print(\"binary columns: \", binary_columns)\n",
    "\n",
    "# The attribute that have multiple value.\n",
    "multiple_columns = []\n",
    "for name, item in traffic_dataset.items():\n",
    "    if len(item.unique()) > 2:\n",
    "        multiple_columns.append(name)\n",
    "print(\"multiple columns: \", multiple_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model\n",
    "We use three datasets to train the model\n",
    "- GET & POST\n",
    "- GET only\n",
    "- POST only\n",
    "\n",
    "For each of the dataset, we used them to train three models.  \n",
    "The number of decision trees in three models are 5, 40, and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_get_post = traffic_dataset[['method', 'host', 'contentType', 'contentLength', 'label']]\n",
    "feat_get_post = feat_get_post.replace({\"method\" : {\"GET\" : 0, \"POST\" : 1}})\n",
    "feat_get_post = feat_get_post.replace({\"host\" : {\"localhost:8080\" : 0, \"localhost:9090\" : 1}})\n",
    "feat_get_post = feat_get_post.replace({\"contentType\" : {\"application/x-www-form-urlencoded\" : 1}})\n",
    "feat_get_post = feat_get_post.replace({\"label\" : {\"norm\" : 0, \"anom\" : 1}})\n",
    "feat_get_post = feat_get_post.fillna(0)\n",
    "\n",
    "feat_all = feat_get_post.drop([\"label\"], axis=1).values\n",
    "y_all = feat_get_post[\"label\"]\n",
    "feat_train, feat_test, y_train, y_test = train_test_split(\n",
    "    feat_all, y_all, test_size=0.4, random_state=0\n",
    ")\n",
    "\n",
    "rf_5_model = RandomForestClassifier(n_estimators=5, n_jobs=N_ENSEMBLE_CPUS)\n",
    "rf_5_model.fit(feat_train, y_train)\n",
    "\n",
    "rf_40_model = RandomForestClassifier(n_estimators=40, n_jobs=N_ENSEMBLE_CPUS)\n",
    "rf_40_model.fit(feat_train, y_train)\n",
    "\n",
    "rf_100_model = RandomForestClassifier(n_jobs=N_ENSEMBLE_CPUS)\n",
    "rf_100_model.fit(feat_train, y_train)\n",
    "\n",
    "evaluate_model(rf_5_model, \"Random forest classifier using GET & POST (5 DTs)\", feat_test, y_test)\n",
    "evaluate_model(rf_40_model, \"Random forest classifier using GET & POST (40 DTs)\", feat_test, y_test)\n",
    "evaluate_model(rf_100_model, \"Random forest classifier using GET & POST (100 DTs)\", feat_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_get = traffic_dataset[['method', 'host', 'contentType', 'contentLength', 'label']]\n",
    "feat_get = feat_get[feat_get[\"method\"] == \"GET\"]\n",
    "feat_get = feat_get.replace({\"method\" : {\"GET\" : 0, \"POST\" : 1}})\n",
    "feat_get = feat_get.replace({\"host\" : {\"localhost:8080\" : 0, \"localhost:9090\" : 1}})\n",
    "feat_get = feat_get.replace({\"contentType\" : {\"application/x-www-form-urlencoded\" : 1}})\n",
    "feat_get = feat_get.replace({\"label\" : {\"norm\" : 0, \"anom\" : 1}})\n",
    "feat_get = feat_get.fillna(0)\n",
    "\n",
    "feat_all = feat_get.drop([\"label\"], axis=1).values\n",
    "y_all = feat_get[\"label\"]\n",
    "feat_train, feat_test, y_train, y_test = train_test_split(\n",
    "    feat_all, y_all, test_size=0.4, random_state=0\n",
    ")\n",
    "\n",
    "rf_5_model = RandomForestClassifier(n_estimators=5, n_jobs=N_ENSEMBLE_CPUS)\n",
    "rf_5_model.fit(feat_train, y_train)\n",
    "\n",
    "rf_40_model = RandomForestClassifier(n_estimators=40, n_jobs=N_ENSEMBLE_CPUS)\n",
    "rf_40_model.fit(feat_train, y_train)\n",
    "\n",
    "rf_100_model = RandomForestClassifier(n_jobs=N_ENSEMBLE_CPUS)\n",
    "rf_100_model.fit(feat_train, y_train)\n",
    "\n",
    "evaluate_model(rf_5_model, \"Random forest classifier using GET (5 DTs)\", feat_test, y_test)\n",
    "evaluate_model(rf_40_model, \"Random forest classifier using GET (40 DTs)\", feat_test, y_test)\n",
    "evaluate_model(rf_100_model, \"Random forest classifier using GET (100 DTs)\", feat_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_post = traffic_dataset[['method', 'host', 'contentType', 'contentLength', 'label']]\n",
    "feat_post = feat_post[feat_post[\"method\"] == \"POST\"]\n",
    "feat_post = feat_post.replace({\"method\" : {\"GET\" : 0, \"POST\" : 1}})\n",
    "feat_post = feat_post.replace({\"host\" : {\"localhost:8080\" : 0, \"localhost:9090\" : 1}})\n",
    "feat_post = feat_post.replace({\"contentType\" : {\"application/x-www-form-urlencoded\" : 1}})\n",
    "feat_post = feat_post.replace({\"label\" : {\"norm\" : 0, \"anom\" : 1}})\n",
    "feat_post = feat_post.fillna(0)\n",
    "\n",
    "feat_all = feat_post.drop([\"label\"], axis=1).values\n",
    "y_all = feat_post[\"label\"]\n",
    "feat_train, feat_test, y_train, y_test = train_test_split(\n",
    "    feat_all, y_all, test_size=0.4, random_state=0\n",
    ")\n",
    "rf_5_model = RandomForestClassifier(n_estimators=5, n_jobs=N_ENSEMBLE_CPUS)\n",
    "rf_5_model.fit(feat_train, y_train)\n",
    "\n",
    "rf_40_model = RandomForestClassifier(n_estimators=40, n_jobs=N_ENSEMBLE_CPUS)\n",
    "rf_40_model.fit(feat_train, y_train)\n",
    "\n",
    "rf_100_model = RandomForestClassifier(n_jobs=N_ENSEMBLE_CPUS)\n",
    "rf_100_model.fit(feat_train, y_train)\n",
    "\n",
    "evaluate_model(rf_5_model, \"Random forest classifier (5 DTs)\", feat_test, y_test)\n",
    "evaluate_model(rf_40_model, \"Random forest classifier (40 DTs)\", feat_test, y_test)\n",
    "evaluate_model(rf_100_model, \"Random forest classifier (100 DTs)\", feat_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'url'\n",
    "\n",
    "# normal_traffic_dataset = pd.read_csv('dataset/normalTrafficTraining.csv', usecols=['method', target, 'label'])\n",
    "# anomalous_traffic_dataset = pd.read_csv(\"dataset/anomalousTrafficTest.csv\", usecols=['method', target, 'label'])\n",
    "\n",
    "normal_traffic_dataset = pd.read_csv('dataset/normalTrafficTraining.csv', usecols=[target, 'label'])\n",
    "anomalous_traffic_dataset = pd.read_csv(\"dataset/anomalousTrafficTest.csv\", usecols=[target, 'label'])\n",
    "\n",
    "normal_target_dataset = normal_traffic_dataset\n",
    "anomalous_target_dataset = anomalous_traffic_dataset\n",
    "\n",
    "normal_target_dataset['label'] = normal_target_dataset['label'].map({'norm': 0})\n",
    "anomalous_target_dataset['label'] = anomalous_target_dataset['label'].map({'anom': 1})\n",
    "\n",
    "# Preview the dataset\n",
    "traffic_dataset = pd.concat([normal_traffic_dataset, anomalous_traffic_dataset])\n",
    "print(traffic_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample will be split into training (60%), validation (20%) and test (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split URLs and labels into training, validation and test sets\n",
    "urls_tv, urls_test, labels_tv, labels_test = train_test_split(\n",
    "    traffic_dataset[target], traffic_dataset[\"label\"], test_size=0.2, random_state=12345\n",
    ")\n",
    "\n",
    "urls_train, urls_vali, labels_train, labels_vali = train_test_split(\n",
    "    urls_tv, labels_tv, test_size=0.25, random_state=12345\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build N-Gram Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_ngrams(text, n, start_symbol=\"\\x00\", end_symbol=\"\\x01\"):\n",
    "    \"\"\" Returns an iterable that yields all N-grams of the text with start and end padding. \"\"\"\n",
    "    padding_text = start_symbol * (n - 1) + text + end_symbol * n\n",
    "    result = []\n",
    "    for i in range(len(text) + n):\n",
    "        result.append(padding_text[i:i + n])\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a bidirectional mapping for all unique N-grams occurring in our dataset.  \n",
    "The function responsible for this step is the following build_ngram_mapping function, which takes a list of urls, n as in N-grams and a rare_threshold under which an N-gram will be marked as rare and discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def build_ngram_mapping(urls, n, rare_threshold=2):\n",
    "    \"\"\" Build bidirectional N-gram to numerical index mapping, ignoring rare N-grams. \"\"\"\n",
    "    # N-gram occurrences count\n",
    "    ngram_count = {} #map\n",
    "    \n",
    "    print(\"Counting occurrences for unique N-grams...\", flush=True)\n",
    "    for url in tqdm(urls):\n",
    "        for ngram in iter_ngrams(url, n):\n",
    "            if ngram in ngram_count:\n",
    "                ngram_count[ngram] += 1\n",
    "            else:\n",
    "                ngram_count[ngram] = 1\n",
    "    \n",
    "    # N-gram to index mapping\n",
    "    n2i_mapping = {}\n",
    "    # Index to N-gram mapping\n",
    "    i2n_mapping = []\n",
    "    \n",
    "    # Total occurrence of all N-grams\n",
    "    n_occurrence = sum(ngram_count.values())\n",
    "    # Total occurrence of rare N-grams\n",
    "    n_occurrence_rare = 0\n",
    "    \n",
    "    print(\"Building bidirectional N-gram to numerical index mapping...\", flush=True)\n",
    "    for ngram, count in tqdm(ngram_count.items()):\n",
    "        if count >= rare_threshold:\n",
    "            n2i_mapping[ngram] = len(n2i_mapping)\n",
    "            i2n_mapping.append(ngram)\n",
    "        else:\n",
    "            n_occurrence_rare += 1\n",
    "    \n",
    "    # Rare occurrence percentage\n",
    "    rare_occurrence_pc = n_occurrence_rare/n_occurrence*100\n",
    "    \n",
    "    # Total number of unique N-grams\n",
    "    n_ngram = len(ngram_count)\n",
    "    # Total number of unique rare N-grams\n",
    "    n_ngram_rare = n_ngram-len(i2n_mapping)\n",
    "    # Rare unique N-grams percentage\n",
    "    rare_ngram_pc = n_ngram_rare/n_ngram*100\n",
    "    \n",
    "    # Print occurrence and unique N-grams metrics\n",
    "    print(f\"Total occurrence: {n_occurrence} ({n_occurrence_rare} rare, {rare_occurrence_pc:.2f}%)\")\n",
    "    print(f\"Unique N-grams: {n_ngram} ({n_ngram_rare} rare, {rare_ngram_pc:.2f}%)\")\n",
    "    \n",
    "    # Return bidirectional mappings\n",
    "    return n2i_mapping, i2n_mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build co-occurrence matrix for all unique N-grams.  \n",
    "This is done by the build_occurrence_matrix below, which takes a list of urls, n as in N-grams, the n2i_mapping we have generated from the last step and a window_size parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "def build_cooccurrence_matrix(urls, n, n2i_mapping, window_size, report_interval=0.05):\n",
    "    \"\"\" Build co-occurrence matrix for character N-grams from URLs. \"\"\"\n",
    "    # Number of unique N-grams / index for unknown symbol\n",
    "    n_ngram = unknown_index = len(n2i_mapping)\n",
    "    # Co-occurrence matrix (DOK sparse matrix)\n",
    "    cooccurrence_matrix = dok_matrix((n_ngram+1, n_ngram+1))\n",
    "    \n",
    "    print(f\"Counting co-occurrence of N-grams (n = {n})...\", flush=True)\n",
    "    # Co-occurrence counting loop\n",
    "    for url in tqdm(urls):\n",
    "        # Previous neighboring N-gram indices for current URL\n",
    "        prev_ngram_indices = []\n",
    "        \n",
    "        for ngram in iter_ngrams(url, n):\n",
    "            ## [ TODO ]\n",
    "            # 1) Get index for current N-gram from mapping\n",
    "            #    (Hint: set `ngram_index` to `unknown_index` if it is not find in mapping)\n",
    "            ngram_index = n2i_mapping[ngram] if ngram in n2i_mapping else unknown_index\n",
    "            \n",
    "            # 2) Update co-occurrence matrix for previous neighboring N-grams in window\n",
    "            for prev_index in prev_ngram_indices:\n",
    "                cooccurrence_matrix[ngram_index, prev_index] += 1\n",
    "                cooccurrence_matrix[prev_index, ngram_index] += 1\n",
    "\n",
    "            # Add current N-gram index\n",
    "            prev_ngram_indices.append(ngram_index)\n",
    "            if len(prev_ngram_indices) > window_size:\n",
    "                prev_ngram_indices.pop(0)\n",
    "\n",
    "    print(\"Normalizing co-occurrence frequencies...\", flush=True)\n",
    "    # Sum of co-occurrence frequencies\n",
    "    co_sum = cooccurrence_matrix.sum(-1)\n",
    "    ## [ TODO ]\n",
    "    # 3) Normalize co-occurrence frequencies into distributions for each row\n",
    "    for i, j in tqdm(cooccurrence_matrix.keys()):\n",
    "        cooccurrence_matrix[i, j] /= co_sum[i]\n",
    "    \n",
    "    print(\"Completed.\")\n",
    "    # Return co-occurrence matrix\n",
    "    return cooccurrence_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a co-occurrence matrix for all URLs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N of N-grams\n",
    "N = 4\n",
    "# Window size for building co-occurrence matrix\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "# Build bidirectional N-gram mapping for all URLs\n",
    "n2i_urls, i2n_urls = build_ngram_mapping(urls_train, N)\n",
    "# Build co-occurrence matrix for character N-grams from URLs\n",
    "cooccurrence_urls = build_cooccurrence_matrix(\n",
    "    urls_train, N, n2i_mapping=n2i_urls, window_size=N\n",
    ")\n",
    "\n",
    "# Review co-occurrence matrix\n",
    "print(type(cooccurrence_urls), cooccurrence_urls.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the TruncatedSVD algorithm to reduce the dimensions of the co-occurrence matrix in order to create an embedding for every unique N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Number of embedding dimensions\n",
    "EMBEDDING_DIMS_SVD = 120\n",
    "\n",
    "svd = TruncatedSVD(n_components=EMBEDDING_DIMS_SVD, n_iter=6, random_state=12345)\n",
    "embedding_svd = svd.fit_transform(cooccurrence_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Build a neural network model for classification. The neural network below is again a Convolutional Neural Network, but it utilizes one-dimensional convolution and max pooling layers because our inputs are one-dimensional sequences rather than two-dimensional images. It also includes an embedding layer that accepts sequences of N-gram indices and outputs the look-up results of N-gram embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "def build_model(n_ngram, embedding_dims, padded_len, n, embedding_weights, dropout_ratio=0., optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "    # 1) Embedding layer: (*, 100) -Embedding-> (*, 100, 10)\n",
    "    model.add(layers.Embedding(n_ngram + 1, embedding_weights.shape[1], input_length=padded_len+n, trainable=False, embeddings_initializer=Constant(embedding_weights))) \n",
    "    # 2) First convolution block: (*, 100, 10) -Conv1D+ReLU-> (*, 100, 20) -MaxPooling1D-> (*, 50, 20)\n",
    "    #    - Convolution layer: 20 channels, kernel size set to 5, ReLU activation, padded to maintain same shape\n",
    "    model.add(layers.Conv1D(20, 5, padding=\"same\", activation=\"relu\"))\n",
    "    #    - Max pooling layer: kernel size and stride set to 2\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "    # 3) Second convolution block: (*, 50, 20) -Conv1D+ReLU-> (*, 50, 40) -MaxPooling1D-> (*, 10, 40)\n",
    "    #    - Convolution layer: 40 channels, kernel size set to 5, ReLU activation, padded to maintain same shape\n",
    "    model.add(layers.Conv1D(40, 5, padding=\"same\", activation=\"relu\"))\n",
    "    #    - Max pooling layer: kernel size and stride set to 5\n",
    "    model.add(layers.MaxPooling1D(pool_size=5, strides=5))\n",
    "    # 4) Third convolution block: (*, 10, 40) -Conv1D+ReLU-> (*, 10, 80) -MaxPooling1D-> (*, 2, 80)\n",
    "    #    - Convolution layer: 80 channels, kernel size set to 5, ReLU activation, padded to maintain same shape\n",
    "    model.add(layers.Conv1D(80, 5, padding=\"same\", activation=\"relu\"))\n",
    "    #    - Max pooling layer: kernel size and stride set to 5\n",
    "    model.add(layers.MaxPooling1D(pool_size=5, strides=5))\n",
    "    # 5) Flatten layer: (*, 2, 80) -Flatten-> (*, 160)\n",
    "    model.add(layers.Flatten())\n",
    "    # 6) First fully-connected layer: (*, 160) -Dense+ReLU-> (*, 16)\n",
    "    #    - Dense layer: ReLU activation\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    # 7) Dropout layer: shape unchanged, dropout ratio set to `dropout_ratio`\n",
    "    model.add(layers.Dropout(dropout_ratio))\n",
    "    # 8) Last fully-connected layer: (*, 16) -Dense+Sigmoid-> (*, 1)\n",
    "    #    - Dense layer: Sigmoid activation\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the classification model\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the neural network model takes sequences of N-gram indices rather than raw URLs as an input. Thus an extra function, transform_urls, is needed to perform conversion between the two formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_urls(batch_urls, padded_len, n, n2i_mapping, pad_symbol=\"\\x01\"):\n",
    "    # Batch size\n",
    "    batch_size = len(batch_urls)\n",
    "    # Batch input to the classification model\n",
    "    batch_input = np.empty((batch_size, padded_len+n), dtype=int)\n",
    "    \n",
    "    # Index for unknown symbol\n",
    "    unknown_index = len(n2i_mapping)\n",
    "    \n",
    "    for i, url in enumerate(batch_urls):\n",
    "        url += pad_symbol * (padded_len - len(url))\n",
    "        for j, ngram in enumerate(iter_ngrams(url, n)):\n",
    "            batch_input[i, j] = n2i_mapping[ngram] if ngram in n2i_mapping else unknown_index\n",
    "    \n",
    "    return batch_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like Keras to call transform_urls for every batch of URLs during training. To achieve this, we will wrap our training and validation set in a SamplesWrapper, which implements the keras.utils.Sequence interface and automatically performs convertion of URLs whenever a new batch of samples is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class SamplesWrapper(Sequence):\n",
    "    \"\"\" Wrapper dataset that performs convertion of URLs on the fly. \"\"\"\n",
    "    def __init__(self, urls, labels, batch_size, padded_len, n, n2i_mapping):\n",
    "        ## URLs\n",
    "        self.urls = urls\n",
    "        ## Labels\n",
    "        self.labels = labels.values.astype(float)\n",
    "        ## Batch size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        ## Length of padded URL\n",
    "        self.padded_len = padded_len\n",
    "        ## N of N-gram\n",
    "        self.n = n\n",
    "        ## N-gram to index mapping\n",
    "        self.n2i_mapping = n2i_mapping\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.urls)/self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, batch_idx):\n",
    "        # Start and end index of batch\n",
    "        batch_start = self.batch_size*batch_idx\n",
    "        batch_end = batch_start+self.batch_size\n",
    "        \n",
    "        # Get and transform batch of URLs\n",
    "        batch_urls = transform_urls(\n",
    "            self.urls[batch_start:batch_end],\n",
    "            self.padded_len,\n",
    "            self.n,\n",
    "            self.n2i_mapping\n",
    "        )\n",
    "        # Get batch of one-hot encoded labels\n",
    "        batch_labels = self.labels[batch_start:batch_end]\n",
    "        \n",
    "        return batch_urls, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for training\n",
    "BATCH_SIZE = 256\n",
    "# N as in N-grams\n",
    "N = 5\n",
    "# Maximum URL length\n",
    "MAX_URL_LEN = 96\n",
    "\n",
    "# Build a basic CNN model\n",
    "model_basic = build_model(\n",
    "    n_ngram=len(n2i_urls),\n",
    "    embedding_dims=EMBEDDING_DIMS_SVD,\n",
    "    padded_len=MAX_URL_LEN,\n",
    "    n=N,\n",
    "    embedding_weights=embedding_svd,\n",
    "    dropout_ratio=0.25\n",
    ")\n",
    "\n",
    "# Wrapper for training samples\n",
    "wrapper_train = SamplesWrapper(\n",
    "    urls_train,\n",
    "    labels_train,\n",
    "    BATCH_SIZE,\n",
    "    padded_len=MAX_URL_LEN,\n",
    "    n=N,\n",
    "    n2i_mapping=n2i_urls\n",
    ")\n",
    "# Wrapper for validation samples\n",
    "wrapper_vali = SamplesWrapper(\n",
    "    urls_vali,\n",
    "    labels_vali,\n",
    "    BATCH_SIZE,\n",
    "    padded_len=MAX_URL_LEN,\n",
    "    n=N,\n",
    "    n2i_mapping=n2i_urls\n",
    ")\n",
    "\n",
    "# Train the basic model for 10 epochs\n",
    "history_basic = model_basic.fit(wrapper_train, validation_data=wrapper_vali, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Plot the loss and accuracy of each epoch on both the training and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history_basic.history[\"accuracy\"])\n",
    "plt.plot(history_basic.history[\"val_accuracy\"])\n",
    "plt.title(\"accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"training set\", \"validation set\"])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_basic.history[\"loss\"])\n",
    "plt.plot(history_basic.history[\"val_loss\"])\n",
    "plt.title(\"loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"training set\", \"validation set\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate the performance of the whole pipeline on the test set:\n",
    "1. Transforming the test URLs into N-gram index sequences by calling the transform_urls function. Except we are transforming all URLs at once, all other parameters of the transform_urls function remain the same.\n",
    "2. The N-gram index sequences are then fed into the neural network model, which computes the log probability of whether the URL is malicious or not.\n",
    "3. The output of the neural network model are then converted back into numerical labels with np.argmax.\n",
    "4. Finally, with both prediction and actual labels, we generate the confusion_matrix and classification_report for the result on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "transformed_urls_test = transform_urls(urls_test, MAX_URL_LEN, N, n2i_urls)\n",
    "\n",
    "# Predict results for test URLs\n",
    "preds_test = model_basic.predict(transformed_urls_test)\n",
    "for preds in preds_test:\n",
    "    for i, pred in enumerate(preds):\n",
    "        preds[i] = 0 if pred < 0.5 else 1\n",
    "        \n",
    "# Confusion matrix\n",
    "print(\"Testing confusion matrix:\\n\", confusion_matrix(labels_test, preds_test), \"\\n\")\n",
    "# Classification report\n",
    "print(\"Testing metrics:\\n\", classification_report(labels_test, preds_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
